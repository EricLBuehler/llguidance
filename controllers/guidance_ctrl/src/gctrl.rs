use aici_abi::{
    arg_bytes, bytes::to_hex_string, tokenize_bytes, toktree::TokTrie, AiciCtrl, MidProcessArg,
    MidProcessResult, TokenId,
};
use base64::{self, Engine as _};
use earley::{earley_grm_from_guidance, Parser};
use serde::{Deserialize, Serialize};

use crate::earley::ParseResult;

mod earley;
mod serialization;

const INFO: bool = true;

macro_rules! infoln {
    ($($arg:tt)*) => {
        if INFO {
            println!($($arg)*);
        }
    };
}

pub struct Runner {
    toktrie: TokTrie,
    parser: Parser,
    // tokens currently in KV cache
    llm_tokens: Vec<TokenId>,
    reported_captures: usize,
}

#[derive(Serialize, Deserialize)]
struct RunnerArg {
    guidance_b64: String,
}

impl Runner {
    pub fn new() -> Self {
        let arg: RunnerArg = serde_json::from_slice(&arg_bytes()).expect("invalid JSON arg");
        let guidance = base64::engine::general_purpose::STANDARD
            .decode(arg.guidance_b64)
            .expect("invalid base64");
        let grm = earley_grm_from_guidance(&guidance).expect("invalid guidance protobuf");
        infoln!("original: {:?}", grm);
        let grm = grm.optimize();
        infoln!("optimized: {:?}", grm);
        let cgrm = grm.compile();
        let parser = Parser::new(cgrm);
        Runner {
            toktrie: TokTrie::from_host(),
            parser,
            llm_tokens: Vec::new(),
            reported_captures: 0,
        }
    }

    fn report_captures(&mut self) {
        let captures = &self.parser.captures()[self.reported_captures..];
        for (name, val) in captures {
            self.reported_captures += 1;
            let cap = Capture {
                object: "capture",
                name: name.clone(),
                str: String::from_utf8_lossy(val).to_string(),
                hex: to_hex_string(val),
            };
            println!("JSON-OUT: {}", serde_json::to_string(&cap).unwrap());
        }
    }
}

#[derive(Serialize, Deserialize)]
struct Capture {
    object: &'static str, // "capture"
    name: String,
    str: String,
    hex: String,
}

impl AiciCtrl for Runner {
    fn mid_process(&mut self, arg: MidProcessArg) -> MidProcessResult {
        infoln!("post tokens: {}", self.toktrie.tokens_dbg(&arg.tokens));
        arg.save_tokens(&mut self.llm_tokens);

        if arg.tokens.contains(&self.toktrie.eos_token()) {
            self.report_captures();
            return MidProcessResult::stop();
        }

        let start_time = std::time::Instant::now();
        let _ = self.parser.force_bytes();
        // tokens/bytes forced by the grammar
        let full_grm_bytes = self.parser.get_bytes();
        let mut grm_tokens = tokenize_bytes(&full_grm_bytes);
        let mut suff = Vec::new();
        let mut chop_tokens = 0;
        let mut chop_bytes = 0;
        for (idx, t) in grm_tokens.iter().rev().enumerate() {
            suff.splice(0..0, self.toktrie.token(*t).iter().cloned());
            if suff.len() > self.toktrie.max_token_len() {
                break;
            }
            // TODO do we have to consider if the extensions match the grammar?
            if self.toktrie.has_extensions(&suff) {
                chop_tokens = idx + 1;
                chop_bytes = suff.len();
            }
        }
        // here we remove a suffix from grm_tokens that could be possibly tokenized differently
        grm_tokens.truncate(grm_tokens.len() - chop_tokens);

        for idx in 0..grm_tokens.len() {
            // if the LLM state disagrees with forced tokens, we need to splice
            if self.llm_tokens.get(idx) != grm_tokens.get(idx) {
                let backtrack: u32 = (self.llm_tokens.len() - idx).try_into().unwrap();
                let ff_tokens = grm_tokens[idx..].to_vec();
                infoln!(
                    "backtrack: {}, ff_tokens: {}",
                    backtrack,
                    self.toktrie.tokens_dbg(&ff_tokens),
                );
                infoln!("fixed_tokens: {:?}", self.toktrie.tokens_dbg(&grm_tokens));
                self.report_captures();
                return MidProcessResult::splice(backtrack, ff_tokens);
            }
        }

        // here, grm_tokens are at most as long as llm_tokens (otherwise we would have spliced)
        // llm_suffix are additional bytes generated by the model
        let llm_suffix = self.toktrie.decode(&self.llm_tokens[grm_tokens.len()..]);
        // grm_suffix are additional bytes generated by the grammar
        let grm_suffix = full_grm_bytes[full_grm_bytes.len() - chop_bytes..].to_vec();

        let byte_suffix = if grm_suffix.len() <= llm_suffix.len() {
            if !llm_suffix.starts_with(&grm_suffix) {
                panic!(
                    "llm_suffix: {:?}, grm_suffix: {:?} (grm<=llm)",
                    String::from_utf8_lossy(&llm_suffix),
                    String::from_utf8_lossy(&grm_suffix)
                );
            }

            for b in &llm_suffix[grm_suffix.len()..] {
                let r = self.parser.scan(*b);
                if r == ParseResult::Reject {
                    panic!("rejected byte: {}", b);
                }
            }
            vec![]
        } else {
            if !grm_suffix.starts_with(&llm_suffix) {
                panic!(
                    "llm_suffix: {:?}, grm_suffix: {:?} (grm>llm)",
                    String::from_utf8_lossy(&llm_suffix),
                    String::from_utf8_lossy(&grm_suffix)
                );
            }
            grm_suffix[llm_suffix.len()..].to_vec()
        };

        // self.parser.print_row(self.parser.num_rows() - 1);

        let mut set = self.toktrie.alloc_token_set();
        self.toktrie
            .compute_bias_ext(&mut self.parser, &mut set, &byte_suffix);
        infoln!(
            "bias: (pref: {:?}) {:?} {}",
            String::from_utf8_lossy(&byte_suffix),
            start_time.elapsed(),
            self.toktrie.token_set_dbg(&set)
        );

        self.report_captures();

        MidProcessResult::sample(set)
    }
}

fn main() {
    #[cfg(not(target_arch = "wasm32"))]
    {
        earley::bench::earley_test(TokTrie::from_host());
    }
}

aici_abi::aici_expose_all!(Runner, Runner::new());
